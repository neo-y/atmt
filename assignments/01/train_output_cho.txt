INFO: COMMAND: ../../train.py --cuda TRUE --data ../../data/en-sv/infopankki/prepared --source-lang sv --target-lang en --log-file ./train_output_cho.txt --save-dir ../../assignments/01/baseline/checkpoints
INFO: Arguments: {'cuda': 'TRUE', 'data': '../../data/en-sv/infopankki/prepared', 'source_lang': 'sv', 'target_lang': 'en', 'max_tokens': None, 'batch_size': 1, 'train_on_tiny': False, 'arch': 'lstm', 'max_epoch': 10000, 'clip_norm': 4.0, 'lr': 0.0003, 'patience': 3, 'log_file': './train_output_cho.txt', 'save_dir': '../../assignments/01/baseline/checkpoints', 'restore_file': 'checkpoint_last.pt', 'save_interval': 1, 'no_save': False, 'epoch_checkpoints': False, 'encoder_embed_dim': 64, 'encoder_embed_path': None, 'encoder_hidden_size': 64, 'encoder_num_layers': 1, 'encoder_bidirectional': 'True', 'encoder_dropout_in': 0.25, 'encoder_dropout_out': 0.25, 'decoder_embed_dim': 64, 'decoder_embed_path': None, 'decoder_hidden_size': 128, 'decoder_num_layers': 1, 'decoder_dropout_in': 0.25, 'decoder_dropout_out': 0.25, 'decoder_use_attention': 'True', 'decoder_use_lexical_model': 'False', 'device_id': 0}
INFO: Loaded a source dictionary (sv) with 4000 words
INFO: Loaded a target dictionary (en) with 4000 words
INFO: Built a model with 1308576 parameters
INFO: Epoch 000: loss 5.18 | lr 0.0003 | num_tokens 14.86 | batch_size 1 | grad_norm 40.82 | clip 0.9961
INFO: Epoch 000: valid_loss 4.86 | num_tokens 15.5 | batch_size 500 | valid_perplexity 129
INFO: Epoch 001: loss 4.395 | lr 0.0003 | num_tokens 14.86 | batch_size 1 | grad_norm 47.35 | clip 0.9984
INFO: Epoch 001: valid_loss 4.41 | num_tokens 15.5 | batch_size 500 | valid_perplexity 82.6
INFO: Epoch 002: loss 4.089 | lr 0.0003 | num_tokens 14.86 | batch_size 1 | grad_norm 52.68 | clip 0.997
INFO: Epoch 002: valid_loss 4.14 | num_tokens 15.5 | batch_size 500 | valid_perplexity 63.1
INFO: Epoch 003: loss 3.878 | lr 0.0003 | num_tokens 14.86 | batch_size 1 | grad_norm 55.61 | clip 0.9961
INFO: Epoch 003: valid_loss 3.97 | num_tokens 15.5 | batch_size 500 | valid_perplexity 53
INFO: Epoch 004: loss 3.713 | lr 0.0003 | num_tokens 14.86 | batch_size 1 | grad_norm 58.03 | clip 0.9954
INFO: Epoch 004: valid_loss 3.84 | num_tokens 15.5 | batch_size 500 | valid_perplexity 46.4
INFO: Epoch 005: loss 3.586 | lr 0.0003 | num_tokens 14.86 | batch_size 1 | grad_norm 59.63 | clip 0.9939
INFO: Epoch 005: valid_loss 3.71 | num_tokens 15.5 | batch_size 500 | valid_perplexity 41
INFO: Epoch 006: loss 3.479 | lr 0.0003 | num_tokens 14.86 | batch_size 1 | grad_norm 61.05 | clip 0.9949
INFO: Epoch 006: valid_loss 3.62 | num_tokens 15.5 | batch_size 500 | valid_perplexity 37.3
INFO: Epoch 007: loss 3.384 | lr 0.0003 | num_tokens 14.86 | batch_size 1 | grad_norm 61.98 | clip 0.9943
INFO: Epoch 007: valid_loss 3.54 | num_tokens 15.5 | batch_size 500 | valid_perplexity 34.5
INFO: Epoch 008: loss 3.297 | lr 0.0003 | num_tokens 14.86 | batch_size 1 | grad_norm 62.71 | clip 0.9937
INFO: Epoch 008: valid_loss 3.46 | num_tokens 15.5 | batch_size 500 | valid_perplexity 31.9
INFO: Epoch 009: loss 3.227 | lr 0.0003 | num_tokens 14.86 | batch_size 1 | grad_norm 63.16 | clip 0.9935
INFO: Epoch 009: valid_loss 3.41 | num_tokens 15.5 | batch_size 500 | valid_perplexity 30.4
INFO: Epoch 010: loss 3.154 | lr 0.0003 | num_tokens 14.86 | batch_size 1 | grad_norm 63.51 | clip 0.9938
INFO: Epoch 010: valid_loss 3.36 | num_tokens 15.5 | batch_size 500 | valid_perplexity 28.8
INFO: Epoch 011: loss 3.095 | lr 0.0003 | num_tokens 14.86 | batch_size 1 | grad_norm 64.04 | clip 0.9933
INFO: Epoch 011: valid_loss 3.32 | num_tokens 15.5 | batch_size 500 | valid_perplexity 27.5
INFO: Epoch 012: loss 3.037 | lr 0.0003 | num_tokens 14.86 | batch_size 1 | grad_norm 64.25 | clip 0.9932
INFO: Epoch 012: valid_loss 3.27 | num_tokens 15.5 | batch_size 500 | valid_perplexity 26.2
INFO: Epoch 013: loss 2.987 | lr 0.0003 | num_tokens 14.86 | batch_size 1 | grad_norm 64.56 | clip 0.993
INFO: Epoch 013: valid_loss 3.24 | num_tokens 15.5 | batch_size 500 | valid_perplexity 25.6
INFO: Epoch 014: loss 2.949 | lr 0.0003 | num_tokens 14.86 | batch_size 1 | grad_norm 64.8 | clip 0.9926
INFO: Epoch 014: valid_loss 3.21 | num_tokens 15.5 | batch_size 500 | valid_perplexity 24.7
INFO: Epoch 015: loss 2.895 | lr 0.0003 | num_tokens 14.86 | batch_size 1 | grad_norm 64.83 | clip 0.9925
INFO: Epoch 015: valid_loss 3.17 | num_tokens 15.5 | batch_size 500 | valid_perplexity 23.9
INFO: Epoch 016: loss 2.855 | lr 0.0003 | num_tokens 14.86 | batch_size 1 | grad_norm 64.98 | clip 0.9922
INFO: Epoch 016: valid_loss 3.14 | num_tokens 15.5 | batch_size 500 | valid_perplexity 23.1
INFO: Epoch 017: loss 2.815 | lr 0.0003 | num_tokens 14.86 | batch_size 1 | grad_norm 64.99 | clip 0.9922
INFO: Epoch 017: valid_loss 3.12 | num_tokens 15.5 | batch_size 500 | valid_perplexity 22.7
INFO: Epoch 018: loss 2.785 | lr 0.0003 | num_tokens 14.86 | batch_size 1 | grad_norm 64.96 | clip 0.9926
INFO: Epoch 018: valid_loss 3.08 | num_tokens 15.5 | batch_size 500 | valid_perplexity 21.8
INFO: Epoch 019: loss 2.749 | lr 0.0003 | num_tokens 14.86 | batch_size 1 | grad_norm 64.92 | clip 0.9917
INFO: Epoch 019: valid_loss 3.06 | num_tokens 15.5 | batch_size 500 | valid_perplexity 21.3
INFO: Epoch 020: loss 2.717 | lr 0.0003 | num_tokens 14.86 | batch_size 1 | grad_norm 64.82 | clip 0.9915
INFO: Epoch 020: valid_loss 3.02 | num_tokens 15.5 | batch_size 500 | valid_perplexity 20.5
INFO: Epoch 021: loss 2.692 | lr 0.0003 | num_tokens 14.86 | batch_size 1 | grad_norm 64.78 | clip 0.9916
INFO: Epoch 021: valid_loss 3.03 | num_tokens 15.5 | batch_size 500 | valid_perplexity 20.6
INFO: Epoch 022: loss 2.661 | lr 0.0003 | num_tokens 14.86 | batch_size 1 | grad_norm 64.95 | clip 0.9917
INFO: Epoch 022: valid_loss 3.02 | num_tokens 15.5 | batch_size 500 | valid_perplexity 20.5
INFO: Epoch 023: loss 2.635 | lr 0.0003 | num_tokens 14.86 | batch_size 1 | grad_norm 64.57 | clip 0.9901
INFO: Epoch 023: valid_loss 3 | num_tokens 15.5 | batch_size 500 | valid_perplexity 20.1
INFO: Epoch 024: loss 2.612 | lr 0.0003 | num_tokens 14.86 | batch_size 1 | grad_norm 64.77 | clip 0.9909
INFO: Epoch 024: valid_loss 2.99 | num_tokens 15.5 | batch_size 500 | valid_perplexity 19.8
INFO: Epoch 025: loss 2.584 | lr 0.0003 | num_tokens 14.86 | batch_size 1 | grad_norm 64.77 | clip 0.9912
INFO: Epoch 025: valid_loss 2.95 | num_tokens 15.5 | batch_size 500 | valid_perplexity 19.1
INFO: Epoch 026: loss 2.571 | lr 0.0003 | num_tokens 14.86 | batch_size 1 | grad_norm 64.72 | clip 0.9908
INFO: Epoch 026: valid_loss 2.93 | num_tokens 15.5 | batch_size 500 | valid_perplexity 18.8
INFO: Epoch 027: loss 2.546 | lr 0.0003 | num_tokens 14.86 | batch_size 1 | grad_norm 64.66 | clip 0.9911
INFO: Epoch 027: valid_loss 2.93 | num_tokens 15.5 | batch_size 500 | valid_perplexity 18.7
INFO: Epoch 028: loss 2.528 | lr 0.0003 | num_tokens 14.86 | batch_size 1 | grad_norm 64.56 | clip 0.9903
INFO: Epoch 028: valid_loss 2.92 | num_tokens 15.5 | batch_size 500 | valid_perplexity 18.5
INFO: Epoch 029: loss 2.511 | lr 0.0003 | num_tokens 14.86 | batch_size 1 | grad_norm 64.44 | clip 0.9906
INFO: Epoch 029: valid_loss 2.91 | num_tokens 15.5 | batch_size 500 | valid_perplexity 18.3
INFO: Epoch 030: loss 2.488 | lr 0.0003 | num_tokens 14.86 | batch_size 1 | grad_norm 64.43 | clip 0.9892
INFO: Epoch 030: valid_loss 2.9 | num_tokens 15.5 | batch_size 500 | valid_perplexity 18.1
INFO: Epoch 031: loss 2.468 | lr 0.0003 | num_tokens 14.86 | batch_size 1 | grad_norm 64.43 | clip 0.9906
INFO: Epoch 031: valid_loss 2.88 | num_tokens 15.5 | batch_size 500 | valid_perplexity 17.8
INFO: Epoch 032: loss 2.456 | lr 0.0003 | num_tokens 14.86 | batch_size 1 | grad_norm 64.47 | clip 0.9909
INFO: Epoch 032: valid_loss 2.87 | num_tokens 15.5 | batch_size 500 | valid_perplexity 17.7
INFO: Epoch 033: loss 2.438 | lr 0.0003 | num_tokens 14.86 | batch_size 1 | grad_norm 64.36 | clip 0.9906
INFO: Epoch 033: valid_loss 2.85 | num_tokens 15.5 | batch_size 500 | valid_perplexity 17.3
INFO: Epoch 034: loss 2.418 | lr 0.0003 | num_tokens 14.86 | batch_size 1 | grad_norm 64.23 | clip 0.9913
INFO: Epoch 034: valid_loss 2.85 | num_tokens 15.5 | batch_size 500 | valid_perplexity 17.2
INFO: Epoch 035: loss 2.405 | lr 0.0003 | num_tokens 14.86 | batch_size 1 | grad_norm 64.11 | clip 0.9906
INFO: Epoch 035: valid_loss 2.83 | num_tokens 15.5 | batch_size 500 | valid_perplexity 17
INFO: Epoch 036: loss 2.391 | lr 0.0003 | num_tokens 14.86 | batch_size 1 | grad_norm 64.06 | clip 0.9907
INFO: Epoch 036: valid_loss 2.82 | num_tokens 15.5 | batch_size 500 | valid_perplexity 16.8
INFO: Epoch 037: loss 2.377 | lr 0.0003 | num_tokens 14.86 | batch_size 1 | grad_norm 64.37 | clip 0.9904
INFO: Epoch 037: valid_loss 2.8 | num_tokens 15.5 | batch_size 500 | valid_perplexity 16.5
INFO: Epoch 038: loss 2.361 | lr 0.0003 | num_tokens 14.86 | batch_size 1 | grad_norm 64.59 | clip 0.9902
INFO: Epoch 038: valid_loss 2.79 | num_tokens 15.5 | batch_size 500 | valid_perplexity 16.3
INFO: Epoch 039: loss 2.347 | lr 0.0003 | num_tokens 14.86 | batch_size 1 | grad_norm 64.46 | clip 0.9904
INFO: Epoch 039: valid_loss 2.79 | num_tokens 15.5 | batch_size 500 | valid_perplexity 16.2
INFO: Epoch 040: loss 2.341 | lr 0.0003 | num_tokens 14.86 | batch_size 1 | grad_norm 64.43 | clip 0.9905
INFO: Epoch 040: valid_loss 2.78 | num_tokens 15.5 | batch_size 500 | valid_perplexity 16.1
INFO: Epoch 041: loss 2.324 | lr 0.0003 | num_tokens 14.86 | batch_size 1 | grad_norm 64.55 | clip 0.9896
INFO: Epoch 041: valid_loss 2.77 | num_tokens 15.5 | batch_size 500 | valid_perplexity 15.9
INFO: Epoch 042: loss 2.314 | lr 0.0003 | num_tokens 14.86 | batch_size 1 | grad_norm 64.32 | clip 0.9907
INFO: Epoch 042: valid_loss 2.75 | num_tokens 15.5 | batch_size 500 | valid_perplexity 15.6
INFO: Epoch 043: loss 2.3 | lr 0.0003 | num_tokens 14.86 | batch_size 1 | grad_norm 64.4 | clip 0.9894
INFO: Epoch 043: valid_loss 2.75 | num_tokens 15.5 | batch_size 500 | valid_perplexity 15.6
INFO: Epoch 044: loss 2.286 | lr 0.0003 | num_tokens 14.86 | batch_size 1 | grad_norm 64.41 | clip 0.99
INFO: Epoch 044: valid_loss 2.75 | num_tokens 15.5 | batch_size 500 | valid_perplexity 15.7
INFO: Epoch 045: loss 2.274 | lr 0.0003 | num_tokens 14.86 | batch_size 1 | grad_norm 64.2 | clip 0.9886
INFO: Epoch 045: valid_loss 2.74 | num_tokens 15.5 | batch_size 500 | valid_perplexity 15.5
INFO: Epoch 046: loss 2.266 | lr 0.0003 | num_tokens 14.86 | batch_size 1 | grad_norm 64.44 | clip 0.9883
INFO: Epoch 046: valid_loss 2.74 | num_tokens 15.5 | batch_size 500 | valid_perplexity 15.5
INFO: Epoch 047: loss 2.257 | lr 0.0003 | num_tokens 14.86 | batch_size 1 | grad_norm 64.51 | clip 0.9883
INFO: Epoch 047: valid_loss 2.75 | num_tokens 15.5 | batch_size 500 | valid_perplexity 15.6
INFO: Epoch 048: loss 2.241 | lr 0.0003 | num_tokens 14.86 | batch_size 1 | grad_norm 64.72 | clip 0.9887
INFO: Epoch 048: valid_loss 2.73 | num_tokens 15.5 | batch_size 500 | valid_perplexity 15.3
INFO: Epoch 049: loss 2.231 | lr 0.0003 | num_tokens 14.86 | batch_size 1 | grad_norm 64.48 | clip 0.9897
INFO: Epoch 049: valid_loss 2.72 | num_tokens 15.5 | batch_size 500 | valid_perplexity 15.2
INFO: Epoch 050: loss 2.224 | lr 0.0003 | num_tokens 14.86 | batch_size 1 | grad_norm 64.62 | clip 0.9888
INFO: Epoch 050: valid_loss 2.71 | num_tokens 15.5 | batch_size 500 | valid_perplexity 15
INFO: Epoch 051: loss 2.217 | lr 0.0003 | num_tokens 14.86 | batch_size 1 | grad_norm 64.68 | clip 0.9877
INFO: Epoch 051: valid_loss 2.7 | num_tokens 15.5 | batch_size 500 | valid_perplexity 14.9
INFO: Epoch 052: loss 2.208 | lr 0.0003 | num_tokens 14.86 | batch_size 1 | grad_norm 64.65 | clip 0.9887
INFO: Epoch 052: valid_loss 2.69 | num_tokens 15.5 | batch_size 500 | valid_perplexity 14.8
INFO: Epoch 053: loss 2.197 | lr 0.0003 | num_tokens 14.86 | batch_size 1 | grad_norm 64.19 | clip 0.9884
INFO: Epoch 053: valid_loss 2.7 | num_tokens 15.5 | batch_size 500 | valid_perplexity 14.8
INFO: Epoch 054: loss 2.187 | lr 0.0003 | num_tokens 14.86 | batch_size 1 | grad_norm 64.58 | clip 0.9878
INFO: Epoch 054: valid_loss 2.71 | num_tokens 15.5 | batch_size 500 | valid_perplexity 15
INFO: Epoch 055: loss 2.186 | lr 0.0003 | num_tokens 14.86 | batch_size 1 | grad_norm 64.74 | clip 0.988
INFO: Epoch 055: valid_loss 2.72 | num_tokens 15.5 | batch_size 500 | valid_perplexity 15.2
INFO: No validation set improvements observed for 3 epochs. Early stop!
